{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aniket/workspace/aniket-codes/2023-10-11 LLMs-AICamp-London/lit-gpt\n"
     ]
    }
   ],
   "source": [
    "%cd lit-gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/aniketmaurya/b8e5bd3f1594bd31ed34375ed916f075/raw/b831e5ab054ac6c94a4a409d32be14b6fdcad82a/databricks-dolly-15k.csv\n",
    "# !mv databricks-dolly-15k.csv /data/aniket/datasets/databricks-dolly-15k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/data/aniket/datasets/databricks-dolly-15k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0         When did Virgin Australia start operating?   \n",
       "1           Which is a species of fish? Tope or Rope   \n",
       "2     Why can camels survive for long without water?   \n",
       "3  Alice's parents have three daughters: Amy, Jes...   \n",
       "4                    When was Tomoaki Komorida born?   \n",
       "\n",
       "                                               input  \\\n",
       "0  Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "\n",
       "                                              output  \n",
       "0  Virgin Australia commenced services on 31 Augu...  \n",
       "1                                               Tope  \n",
       "2  Camels use the fat in their humps to keep them...  \n",
       "3            The name of the third daughter is Alice  \n",
       "4         Tomoaki Komorida was born on July 10,1981.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"instruction\", \"input\", \"output\"]\n",
    "df[COLUMNS].to_csv(\"databricks-dolly-15k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "train has 13,510 samples\n",
      "test has 1,501 samples\n",
      "Processing train split ...\n",
      "100%|███████████████████████████████████| 13510/13510 [00:12<00:00, 1113.75it/s]\n",
      "Processing test split ...\n",
      "100%|█████████████████████████████████████| 1501/1501 [00:01<00:00, 1141.28it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/prepare_csv.py \\\n",
    "    --csv_path \"databricks-dolly-15k.csv\" \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --destination_path \"data/dolly\" \\\n",
    "    --max_seq_length 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 1, 'gradient_accumulation_iters': 128, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model '/data/aniket/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 11008, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Number of trainable parameters: 4,194,304\n",
      "Number of non trainable parameters: 6,738,415,616\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "I recommend The Little Prince because, in my opinion, it is one of the most thoughtful and original movies ever made. The Little Prince speaks to us on a profound level because it deals with the question of what makes life worth living and how we can remain open to the world. The Little Prince is a timeless classic that is sure to appeal to your interests.\n",
      "\n",
      "### Explanation:\n",
      "* The Little Prince is a visually stunning film that\n",
      "Estimated TFLOPs: 154.49\n",
      "Measured TFLOPs: 13.91\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-11 LLMs-AICamp-London/lit-gpt/finetune/lora.py\", line 335, in <module>\n",
      "    CLI(setup)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 96, in CLI\n",
      "    return _run_component(components, cfg_init)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 181, in _run_component\n",
      "    return component(**cfg)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-11 LLMs-AICamp-London/lit-gpt/finetune/lora.py\", line 96, in setup\n",
      "    fabric.launch(main, data_dir, checkpoint_dir, out_dir, quantize)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-11 LLMs-AICamp-London/lit-gpt/finetune/lora.py\", line 153, in main\n",
      "    train(fabric, model, optimizer, scheduler, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-11 LLMs-AICamp-London/lit-gpt/finetune/lora.py\", line 216, in train\n",
      "    logits = model(input_ids, lm_head_chunk_size=128)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/wrappers.py\", line 121, in forward\n",
      "    output = self._forward_module(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lit_gpt/lora.py\", line 498, in forward\n",
      "    x = block(x, cos, sin, mask, input_pos)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lit_gpt/model.py\", line 169, in forward\n",
      "    x = x + self.mlp(self.norm_2(x))\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lit_gpt/model.py\", line 303, in forward\n",
      "    x_fc_2 = self.fc_2(x)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lit_gpt/lora.py\", line 146, in forward\n",
      "    pretrained = self.linear(x)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 69.12 MiB is free. Including non-PyTorch memory, this process has 39.32 GiB memory in use. Of the allocated memory 38.68 GiB is allocated by PyTorch, and 130.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 1, 'gradient_accumulation_iters': 128, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model '/data/aniket/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 11008, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Number of trainable parameters: 4,194,304\n",
      "Number of non trainable parameters: 6,738,415,616\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "I recommend The Little Prince because it is an animated movie based on one of the most famous novels in history. The movie is very sweet and speaks about how to be adult and live life to the fullest. The Little Prince movie is a great way to bond with someone and become closer to them.\n",
      " use Test::More;\n",
      "\n",
      "use_ok( 'JSON::XS' );\n",
      "\n",
      "ok( my $obj = JSON::XS->new->utf\n",
      "Estimated TFLOPs: 154.49\n",
      "Measured TFLOPs: 13.91\n",
      "iter 0 step 0: loss 1.4201, iter time: 1696.10ms\n",
      "iter 1 step 0: loss 1.1957, iter time: 193.77ms\n",
      "iter 2 step 0: loss 2.2911, iter time: 222.67ms\n",
      "iter 3 step 0: loss 3.1483, iter time: 148.07ms\n",
      "iter 4 step 0: loss 1.4337, iter time: 141.99ms\n",
      "iter 5 step 0: loss 2.0145, iter time: 156.39ms\n",
      "iter 6 step 0: loss 2.2747, iter time: 145.15ms\n",
      "iter 7 step 0: loss 1.9031, iter time: 150.91ms\n",
      "iter 8 step 0: loss 1.5308, iter time: 125.39ms\n",
      "iter 9 step 0: loss 2.6530, iter time: 132.54ms\n",
      "iter 10 step 0: loss 1.4631, iter time: 131.13ms\n",
      "iter 11 step 0: loss 2.3826, iter time: 127.00ms\n",
      "iter 12 step 0: loss 1.9010, iter time: 161.06ms\n",
      "iter 13 step 0: loss 2.0136, iter time: 137.04ms\n",
      "iter 14 step 0: loss 2.7185, iter time: 129.84ms\n",
      "iter 15 step 0: loss 2.2943, iter time: 131.16ms\n",
      "iter 16 step 0: loss 2.5771, iter time: 128.35ms\n",
      "iter 17 step 0: loss 2.2940, iter time: 136.95ms\n",
      "iter 18 step 0: loss 1.9907, iter time: 150.10ms\n",
      "iter 19 step 0: loss 1.4631, iter time: 137.68ms\n",
      "iter 20 step 0: loss 2.0148, iter time: 136.97ms\n",
      "iter 21 step 0: loss 2.1613, iter time: 131.57ms\n",
      "iter 22 step 0: loss 2.9290, iter time: 129.88ms\n",
      "iter 23 step 0: loss 2.1041, iter time: 129.21ms\n",
      "iter 24 step 0: loss 2.8487, iter time: 146.19ms\n",
      "iter 25 step 0: loss 2.0757, iter time: 148.53ms\n",
      "iter 26 step 0: loss 2.2578, iter time: 176.93ms\n",
      "iter 27 step 0: loss 2.7139, iter time: 173.51ms\n",
      "iter 28 step 0: loss 2.2709, iter time: 191.78ms\n",
      "iter 29 step 0: loss 2.2945, iter time: 149.08ms\n",
      "iter 30 step 0: loss 2.1444, iter time: 175.57ms\n",
      "iter 31 step 0: loss 0.9833, iter time: 161.76ms\n",
      "iter 32 step 0: loss 2.0355, iter time: 168.05ms\n",
      "iter 33 step 0: loss 1.7466, iter time: 159.89ms\n",
      "iter 34 step 0: loss 2.1497, iter time: 153.76ms\n",
      "iter 35 step 0: loss 1.7859, iter time: 173.28ms\n",
      "iter 36 step 0: loss 1.9107, iter time: 171.43ms\n",
      "iter 37 step 0: loss 2.5421, iter time: 161.84ms\n",
      "iter 38 step 0: loss 1.3384, iter time: 180.73ms\n",
      "iter 39 step 0: loss 1.2687, iter time: 167.23ms\n",
      "iter 40 step 0: loss 2.3863, iter time: 152.10ms\n",
      "iter 41 step 0: loss 2.4877, iter time: 175.17ms\n",
      "iter 42 step 0: loss 2.1067, iter time: 163.45ms\n",
      "iter 43 step 0: loss 2.5300, iter time: 173.73ms\n",
      "iter 44 step 0: loss 2.6983, iter time: 153.04ms\n",
      "iter 45 step 0: loss 1.6279, iter time: 163.46ms\n",
      "iter 46 step 0: loss 3.0707, iter time: 164.41ms\n",
      "iter 47 step 0: loss 2.6496, iter time: 158.87ms\n",
      "iter 48 step 0: loss 2.0001, iter time: 152.22ms\n",
      "iter 49 step 0: loss 1.7813, iter time: 160.19ms\n",
      "iter 50 step 0: loss 2.9013, iter time: 141.86ms\n",
      "iter 51 step 0: loss 2.5193, iter time: 144.47ms\n",
      "iter 52 step 0: loss 2.4638, iter time: 160.31ms\n",
      "iter 53 step 0: loss 2.8120, iter time: 176.72ms\n",
      "iter 54 step 0: loss 2.1474, iter time: 161.06ms\n",
      "iter 55 step 0: loss 2.0700, iter time: 171.03ms\n",
      "iter 56 step 0: loss 2.0994, iter time: 174.56ms\n",
      "iter 57 step 0: loss 1.5727, iter time: 161.98ms\n",
      "iter 58 step 0: loss 1.2440, iter time: 159.34ms\n",
      "iter 59 step 0: loss 2.1902, iter time: 154.87ms\n",
      "iter 60 step 0: loss 1.8275, iter time: 165.69ms\n",
      "iter 61 step 0: loss 2.3614, iter time: 183.75ms\n",
      "iter 62 step 0: loss 2.0428, iter time: 163.78ms\n",
      "iter 63 step 0: loss 1.8893, iter time: 175.33ms\n",
      "iter 64 step 0: loss 1.6067, iter time: 175.34ms\n",
      "iter 65 step 0: loss 1.7302, iter time: 159.65ms\n",
      "iter 66 step 0: loss 2.3734, iter time: 161.85ms\n",
      "iter 67 step 0: loss 2.9007, iter time: 168.05ms\n",
      "iter 68 step 0: loss 2.1261, iter time: 164.57ms\n",
      "iter 69 step 0: loss 2.0393, iter time: 159.86ms\n",
      "iter 70 step 0: loss 2.6967, iter time: 164.28ms\n",
      "iter 71 step 0: loss 2.4310, iter time: 170.10ms\n",
      "iter 72 step 0: loss 2.1671, iter time: 168.94ms\n",
      "iter 73 step 0: loss 2.5082, iter time: 149.96ms\n",
      "iter 74 step 0: loss 2.1583, iter time: 161.48ms\n",
      "iter 75 step 0: loss 2.2361, iter time: 145.20ms\n",
      "iter 76 step 0: loss 2.3480, iter time: 169.12ms\n",
      "iter 77 step 0: loss 2.4142, iter time: 154.49ms\n",
      "iter 78 step 0: loss 2.2217, iter time: 168.84ms\n",
      "iter 79 step 0: loss 2.1784, iter time: 188.06ms\n",
      "iter 80 step 0: loss 1.6545, iter time: 159.80ms\n",
      "iter 81 step 0: loss 2.4373, iter time: 157.06ms\n",
      "iter 82 step 0: loss 1.4582, iter time: 161.06ms\n",
      "iter 83 step 0: loss 2.0935, iter time: 146.44ms\n",
      "iter 84 step 0: loss 2.7458, iter time: 159.13ms\n",
      "iter 85 step 0: loss 2.0927, iter time: 142.03ms\n",
      "iter 86 step 0: loss 2.6335, iter time: 129.28ms\n",
      "iter 87 step 0: loss 1.7890, iter time: 129.76ms\n",
      "iter 88 step 0: loss 1.2941, iter time: 137.12ms\n",
      "iter 89 step 0: loss 2.1007, iter time: 125.79ms\n",
      "iter 90 step 0: loss 2.8909, iter time: 139.39ms\n",
      "iter 91 step 0: loss 2.8717, iter time: 134.23ms\n",
      "iter 92 step 0: loss 2.3728, iter time: 151.88ms\n",
      "iter 93 step 0: loss 1.9070, iter time: 131.28ms\n",
      "iter 94 step 0: loss 2.8390, iter time: 135.32ms\n",
      "iter 95 step 0: loss 1.9028, iter time: 132.49ms\n",
      "iter 96 step 0: loss 2.1317, iter time: 130.76ms\n",
      "iter 97 step 0: loss 2.7084, iter time: 136.43ms\n",
      "iter 98 step 0: loss 2.4255, iter time: 145.80ms\n",
      "iter 99 step 0: loss 1.3653, iter time: 134.07ms\n",
      "iter 100 step 0: loss 2.1781, iter time: 131.94ms\n",
      "iter 101 step 0: loss 2.0157, iter time: 147.34ms\n",
      "iter 102 step 0: loss 2.4362, iter time: 130.23ms\n",
      "iter 103 step 0: loss 2.5701, iter time: 129.62ms\n",
      "iter 104 step 0: loss 2.2237, iter time: 140.34ms\n",
      "iter 105 step 0: loss 2.1881, iter time: 131.10ms\n",
      "iter 106 step 0: loss 1.3419, iter time: 131.79ms\n",
      "iter 107 step 0: loss 1.9428, iter time: 153.88ms\n",
      "iter 108 step 0: loss 1.4023, iter time: 143.02ms\n",
      "iter 109 step 0: loss 1.2541, iter time: 130.71ms\n",
      "iter 110 step 0: loss 1.9155, iter time: 136.23ms\n",
      "iter 111 step 0: loss 1.5407, iter time: 141.29ms\n",
      "iter 112 step 0: loss 2.5657, iter time: 141.95ms\n",
      "iter 113 step 0: loss 2.7654, iter time: 130.98ms\n",
      "iter 114 step 0: loss 1.8464, iter time: 139.30ms\n",
      "iter 115 step 0: loss 1.8239, iter time: 134.61ms\n",
      "iter 116 step 0: loss 2.1924, iter time: 150.78ms\n",
      "iter 117 step 0: loss 1.2856, iter time: 134.00ms\n",
      "iter 118 step 0: loss 1.5514, iter time: 133.43ms\n",
      "iter 119 step 0: loss 2.6729, iter time: 141.13ms\n",
      "iter 120 step 0: loss 2.4463, iter time: 126.44ms\n",
      "iter 121 step 0: loss 1.8385, iter time: 155.20ms\n",
      "iter 122 step 0: loss 1.7499, iter time: 140.41ms\n",
      "iter 123 step 0: loss 1.8163, iter time: 139.45ms\n",
      "iter 124 step 0: loss 3.0448, iter time: 138.26ms\n",
      "iter 125 step 0: loss 2.4472, iter time: 134.60ms\n",
      "iter 126 step 0: loss 1.8329, iter time: 136.36ms\n",
      "iter 127 step 1: loss 2.1178, iter time: 192.21ms (optimizer.step)\n",
      "iter 128 step 1: loss 1.2143, iter time: 161.55ms\n",
      "iter 129 step 1: loss 1.9456, iter time: 131.86ms\n",
      "iter 130 step 1: loss 3.1880, iter time: 136.71ms\n",
      "iter 131 step 1: loss 2.9384, iter time: 133.81ms\n",
      "iter 132 step 1: loss 3.0172, iter time: 172.01ms\n",
      "iter 133 step 1: loss 2.2582, iter time: 149.84ms\n",
      "iter 134 step 1: loss 2.7273, iter time: 172.34ms\n",
      "iter 135 step 1: loss 2.7998, iter time: 143.12ms\n",
      "iter 136 step 1: loss 3.1002, iter time: 140.51ms\n",
      "iter 137 step 1: loss 1.8821, iter time: 147.22ms\n",
      "iter 138 step 1: loss 2.4674, iter time: 152.96ms\n",
      "iter 139 step 1: loss 1.2433, iter time: 150.80ms\n",
      "iter 140 step 1: loss 2.0925, iter time: 176.70ms\n",
      "iter 141 step 1: loss 1.9416, iter time: 213.32ms\n",
      "iter 142 step 1: loss 2.1008, iter time: 163.71ms\n",
      "iter 143 step 1: loss 2.8017, iter time: 181.73ms\n",
      "iter 144 step 1: loss 2.3348, iter time: 158.54ms\n",
      "iter 145 step 1: loss 2.6135, iter time: 191.34ms\n",
      "iter 146 step 1: loss 1.9216, iter time: 212.29ms\n",
      "iter 147 step 1: loss 2.2084, iter time: 161.12ms\n",
      "iter 148 step 1: loss 2.2993, iter time: 173.09ms\n",
      "iter 149 step 1: loss 1.0212, iter time: 224.13ms\n",
      "iter 150 step 1: loss 2.9247, iter time: 173.09ms\n",
      "iter 151 step 1: loss 1.3929, iter time: 188.16ms\n",
      "iter 152 step 1: loss 2.8894, iter time: 145.27ms\n",
      "iter 153 step 1: loss 2.6592, iter time: 155.85ms\n",
      "iter 154 step 1: loss 1.7121, iter time: 219.65ms\n",
      "iter 155 step 1: loss 3.0758, iter time: 174.05ms\n",
      "iter 156 step 1: loss 1.8545, iter time: 167.07ms\n",
      "iter 157 step 1: loss 2.0211, iter time: 165.44ms\n",
      "iter 158 step 1: loss 1.1335, iter time: 154.90ms\n",
      "iter 159 step 1: loss 1.0603, iter time: 151.69ms\n",
      "iter 160 step 1: loss 2.8514, iter time: 173.98ms\n",
      "iter 161 step 1: loss 2.0466, iter time: 162.88ms\n",
      "iter 162 step 1: loss 1.8639, iter time: 178.28ms\n",
      "iter 163 step 1: loss 2.0050, iter time: 158.32ms\n",
      "iter 164 step 1: loss 1.2320, iter time: 173.34ms\n",
      "iter 165 step 1: loss 2.1012, iter time: 163.11ms\n",
      "iter 166 step 1: loss 2.4717, iter time: 151.38ms\n",
      "iter 167 step 1: loss 2.2652, iter time: 147.58ms\n",
      "iter 168 step 1: loss 2.5502, iter time: 139.90ms\n",
      "iter 169 step 1: loss 1.2216, iter time: 133.32ms\n",
      "iter 170 step 1: loss 2.2286, iter time: 138.68ms\n",
      "iter 171 step 1: loss 3.2959, iter time: 134.43ms\n",
      "iter 172 step 1: loss 2.1301, iter time: 159.21ms\n",
      "iter 173 step 1: loss 3.1259, iter time: 145.04ms\n",
      "iter 174 step 1: loss 2.2999, iter time: 166.12ms\n",
      "iter 175 step 1: loss 0.7896, iter time: 174.02ms\n",
      "iter 176 step 1: loss 2.1733, iter time: 151.60ms\n",
      "iter 177 step 1: loss 1.1826, iter time: 154.18ms\n",
      "iter 178 step 1: loss 2.9559, iter time: 177.08ms\n",
      "iter 179 step 1: loss 2.1112, iter time: 156.23ms\n",
      "iter 180 step 1: loss 2.0136, iter time: 153.14ms\n",
      "iter 181 step 1: loss 2.9022, iter time: 135.42ms\n",
      "iter 182 step 1: loss 1.9868, iter time: 138.90ms\n",
      "iter 183 step 1: loss 2.7760, iter time: 129.16ms\n",
      "iter 184 step 1: loss 2.2356, iter time: 136.51ms\n",
      "iter 185 step 1: loss 2.7507, iter time: 139.68ms\n",
      "iter 186 step 1: loss 2.1430, iter time: 151.15ms\n",
      "iter 187 step 1: loss 2.1573, iter time: 172.12ms\n",
      "iter 188 step 1: loss 1.6849, iter time: 156.54ms\n",
      "iter 189 step 1: loss 2.0466, iter time: 165.67ms\n",
      "iter 190 step 1: loss 1.9502, iter time: 147.47ms\n",
      "iter 191 step 1: loss 2.6237, iter time: 130.60ms\n",
      "iter 192 step 1: loss 1.7781, iter time: 188.39ms\n",
      "iter 193 step 1: loss 2.1834, iter time: 161.63ms\n",
      "iter 194 step 1: loss 1.8394, iter time: 146.40ms\n",
      "iter 195 step 1: loss 2.5132, iter time: 171.07ms\n",
      "iter 196 step 1: loss 2.8229, iter time: 140.81ms\n",
      "iter 197 step 1: loss 2.2223, iter time: 159.02ms\n",
      "iter 198 step 1: loss 2.0709, iter time: 169.78ms\n",
      "iter 199 step 1: loss 3.2813, iter time: 134.94ms\n",
      "iter 200 step 1: loss 2.4531, iter time: 146.94ms\n",
      "iter 201 step 1: loss 1.7650, iter time: 156.39ms\n",
      "iter 202 step 1: loss 1.9202, iter time: 153.37ms\n",
      "iter 203 step 1: loss 2.1466, iter time: 164.37ms\n",
      "iter 204 step 1: loss 1.6016, iter time: 165.45ms\n",
      "iter 205 step 1: loss 2.3036, iter time: 160.51ms\n",
      "iter 206 step 1: loss 1.5267, iter time: 181.57ms\n",
      "iter 207 step 1: loss 2.6967, iter time: 136.70ms\n",
      "iter 208 step 1: loss 2.8282, iter time: 166.57ms\n",
      "iter 209 step 1: loss 1.8383, iter time: 146.98ms\n",
      "iter 210 step 1: loss 2.6151, iter time: 138.66ms\n",
      "iter 211 step 1: loss 1.8090, iter time: 156.62ms\n",
      "iter 212 step 1: loss 2.0680, iter time: 135.10ms\n",
      "iter 213 step 1: loss 2.8059, iter time: 152.56ms\n",
      "iter 214 step 1: loss 1.2738, iter time: 162.60ms\n",
      "iter 215 step 1: loss 1.9352, iter time: 156.41ms\n",
      "iter 216 step 1: loss 2.1890, iter time: 162.91ms\n",
      "iter 217 step 1: loss 1.4024, iter time: 170.27ms\n",
      "iter 218 step 1: loss 1.8979, iter time: 152.06ms\n",
      "iter 219 step 1: loss 2.1179, iter time: 146.10ms\n",
      "iter 220 step 1: loss 2.7783, iter time: 150.70ms\n",
      "iter 221 step 1: loss 2.0635, iter time: 155.45ms\n",
      "iter 222 step 1: loss 1.4860, iter time: 143.87ms\n",
      "iter 223 step 1: loss 2.4178, iter time: 136.55ms\n",
      "iter 224 step 1: loss 1.6380, iter time: 164.97ms\n",
      "iter 225 step 1: loss 1.6213, iter time: 136.69ms\n",
      "iter 226 step 1: loss 1.8426, iter time: 143.14ms\n",
      "iter 227 step 1: loss 1.9183, iter time: 150.16ms\n",
      "iter 228 step 1: loss 2.0048, iter time: 154.24ms\n",
      "iter 229 step 1: loss 2.1350, iter time: 148.58ms\n",
      "iter 230 step 1: loss 2.0103, iter time: 140.85ms\n",
      "iter 231 step 1: loss 2.5132, iter time: 137.77ms\n",
      "iter 232 step 1: loss 2.8487, iter time: 142.80ms\n",
      "iter 233 step 1: loss 3.0703, iter time: 149.04ms\n",
      "iter 234 step 1: loss 1.4818, iter time: 194.39ms\n",
      "iter 235 step 1: loss 2.1925, iter time: 172.93ms\n",
      "iter 236 step 1: loss 1.8300, iter time: 161.50ms\n",
      "iter 237 step 1: loss 2.8462, iter time: 135.50ms\n",
      "iter 238 step 1: loss 2.5384, iter time: 150.82ms\n",
      "iter 239 step 1: loss 1.7941, iter time: 161.74ms\n",
      "iter 240 step 1: loss 2.6542, iter time: 150.78ms\n",
      "iter 241 step 1: loss 2.5936, iter time: 142.60ms\n",
      "iter 242 step 1: loss 1.8015, iter time: 143.88ms\n",
      "iter 243 step 1: loss 2.0376, iter time: 146.34ms\n",
      "iter 244 step 1: loss 1.5998, iter time: 159.20ms\n",
      "iter 245 step 1: loss 1.7622, iter time: 168.69ms\n",
      "iter 246 step 1: loss 1.1848, iter time: 181.88ms\n",
      "iter 247 step 1: loss 2.6670, iter time: 145.00ms\n",
      "iter 248 step 1: loss 1.9337, iter time: 138.67ms\n",
      "iter 249 step 1: loss 1.5569, iter time: 151.02ms\n",
      "iter 250 step 1: loss 1.6397, iter time: 165.35ms\n",
      "iter 251 step 1: loss 1.7337, iter time: 155.60ms\n",
      "iter 252 step 1: loss 2.8604, iter time: 166.49ms\n",
      "iter 253 step 1: loss 2.6338, iter time: 159.87ms\n",
      "iter 254 step 1: loss 2.7272, iter time: 158.35ms\n",
      "iter 255 step 2: loss 0.8428, iter time: 150.48ms (optimizer.step)\n",
      "iter 256 step 2: loss 1.6389, iter time: 169.67ms\n",
      "iter 257 step 2: loss 1.9537, iter time: 136.32ms\n",
      "iter 258 step 2: loss 2.5862, iter time: 163.42ms\n",
      "iter 259 step 2: loss 1.8845, iter time: 146.53ms\n",
      "iter 260 step 2: loss 1.5742, iter time: 151.90ms\n",
      "iter 261 step 2: loss 1.8837, iter time: 162.50ms\n",
      "iter 262 step 2: loss 3.2510, iter time: 143.68ms\n",
      "iter 263 step 2: loss 1.0743, iter time: 143.53ms\n",
      "iter 264 step 2: loss 2.1096, iter time: 137.22ms\n",
      "iter 265 step 2: loss 1.9152, iter time: 141.60ms\n",
      "iter 266 step 2: loss 1.8476, iter time: 166.41ms\n",
      "iter 267 step 2: loss 2.3428, iter time: 166.76ms\n",
      "iter 268 step 2: loss 2.4897, iter time: 157.20ms\n",
      "iter 269 step 2: loss 2.3480, iter time: 152.75ms\n",
      "iter 270 step 2: loss 3.5522, iter time: 158.70ms\n",
      "iter 271 step 2: loss 0.9448, iter time: 147.65ms\n",
      "iter 272 step 2: loss 2.3471, iter time: 156.91ms\n",
      "iter 273 step 2: loss 1.6217, iter time: 156.55ms\n",
      "iter 274 step 2: loss 2.0534, iter time: 147.07ms\n",
      "iter 275 step 2: loss 1.4892, iter time: 152.59ms\n",
      "iter 276 step 2: loss 1.9484, iter time: 132.92ms\n",
      "iter 277 step 2: loss 1.3957, iter time: 134.63ms\n",
      "iter 278 step 2: loss 2.2568, iter time: 170.06ms\n",
      "iter 279 step 2: loss 2.3977, iter time: 149.75ms\n",
      "iter 280 step 2: loss 3.2821, iter time: 148.73ms\n",
      "iter 281 step 2: loss 2.4293, iter time: 140.78ms\n",
      "iter 282 step 2: loss 1.5107, iter time: 148.49ms\n",
      "iter 283 step 2: loss 1.5912, iter time: 174.33ms\n",
      "iter 284 step 2: loss 2.2568, iter time: 165.23ms\n",
      "iter 285 step 2: loss 2.2310, iter time: 141.61ms\n",
      "iter 286 step 2: loss 2.7536, iter time: 152.54ms\n",
      "iter 287 step 2: loss 2.3783, iter time: 155.98ms\n",
      "iter 288 step 2: loss 2.3443, iter time: 129.47ms\n",
      "iter 289 step 2: loss 2.8926, iter time: 148.04ms\n",
      "iter 290 step 2: loss 1.3204, iter time: 158.32ms\n",
      "iter 291 step 2: loss 2.9716, iter time: 134.58ms\n",
      "iter 292 step 2: loss 2.8554, iter time: 140.59ms\n",
      "iter 293 step 2: loss 2.1352, iter time: 137.36ms\n",
      "iter 294 step 2: loss 2.5453, iter time: 133.45ms\n",
      "iter 295 step 2: loss 3.1185, iter time: 140.41ms\n",
      "iter 296 step 2: loss 1.8642, iter time: 155.64ms\n",
      "iter 297 step 2: loss 2.2295, iter time: 139.17ms\n",
      "iter 298 step 2: loss 2.7465, iter time: 127.39ms\n",
      "iter 299 step 2: loss 1.5660, iter time: 144.51ms\n",
      "iter 300 step 2: loss 1.5167, iter time: 159.51ms\n",
      "iter 301 step 2: loss 2.2359, iter time: 137.79ms\n",
      "iter 302 step 2: loss 1.7114, iter time: 158.96ms\n",
      "iter 303 step 2: loss 1.6712, iter time: 163.04ms\n",
      "iter 304 step 2: loss 1.6254, iter time: 168.92ms\n",
      "iter 305 step 2: loss 2.1928, iter time: 163.93ms\n",
      "iter 306 step 2: loss 2.3286, iter time: 145.50ms\n",
      "iter 307 step 2: loss 2.6548, iter time: 148.17ms\n",
      "iter 308 step 2: loss 1.7440, iter time: 146.88ms\n",
      "iter 309 step 2: loss 2.7051, iter time: 156.56ms\n",
      "iter 310 step 2: loss 1.6237, iter time: 148.96ms\n",
      "iter 311 step 2: loss 1.5512, iter time: 151.00ms\n",
      "iter 312 step 2: loss 2.2531, iter time: 146.08ms\n",
      "iter 313 step 2: loss 3.0713, iter time: 160.96ms\n",
      "iter 314 step 2: loss 1.3985, iter time: 134.49ms\n",
      "iter 315 step 2: loss 2.3905, iter time: 140.45ms\n",
      "iter 316 step 2: loss 2.3783, iter time: 156.64ms\n",
      "iter 317 step 2: loss 1.6232, iter time: 171.67ms\n",
      "iter 318 step 2: loss 2.4773, iter time: 145.29ms\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\" \\\n",
    "    --precision bf16-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\" \\\n",
    "    --precision bf16-true \\\n",
    "    --quantize bnb.fp4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/merge_lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --lora_path \"out/dolly/Llama-2-7b-hf/lit_model_lora_finetuned.pth\" \\\n",
    "    --out_dir \"out/dolly/Llama-2-7b-hf/\"\n",
    "\n",
    "\n",
    "!python eval/lm_eval_harness.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --eval_tasks \"[truthfulqa_mc]\" \\\n",
    "    --precision \"bf16-true\" \\\n",
    "    --batch_size 4 \\\n",
    "    --save_filepath \"results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
